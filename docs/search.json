[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I will write something. I promise."
  },
  {
    "objectID": "posts/when-to-use-probabilistic-forecasts-for-bidding/when_to_use_probabilistic_forecasts_for_bidding.html",
    "href": "posts/when-to-use-probabilistic-forecasts-for-bidding/when_to_use_probabilistic_forecasts_for_bidding.html",
    "title": "When to use probabilistic forecasts for bidding in power markets",
    "section": "",
    "text": "The decision to use probabilistic forecasts and stochastic optimization in bidding for power markets, such as the day-ahead electricity market, is a subject of ongoing debate. The most common argument against it is that it adds complexity to an already complex decision making process. There are also more trivial reasons like the absence of scenarios or any kind of uncertainty forecasts. Despite these challenges, the primary advantage of stochastic optimization – its superior performance when paired with reliable probabilistic forecasts* – is undeniable. There are publications that quantify how much better stochastic optimization is compared to a deterministic approach for unit-commitment and bidding problems. However, what was somewhat unclear to me is when you actually need probabilistic forecasts and which one until I found this paper [2]. This blog post aims to concisely review the “Bidding Curve Optimization” section from the insightful paper titled “Bidding and Scheduling in Energy Markets: Which Probabilistic Forecast Do We Need?”.\n\n* It’s not that hard, check out my paper for creating time-coupled wind power scenarios [1]."
  },
  {
    "objectID": "posts/when-to-use-probabilistic-forecasts-for-bidding/when_to_use_probabilistic_forecasts_for_bidding.html#introduction",
    "href": "posts/when-to-use-probabilistic-forecasts-for-bidding/when_to_use_probabilistic_forecasts_for_bidding.html#introduction",
    "title": "When to use probabilistic forecasts for bidding in power markets",
    "section": "",
    "text": "The decision to use probabilistic forecasts and stochastic optimization in bidding for power markets, such as the day-ahead electricity market, is a subject of ongoing debate. The most common argument against it is that it adds complexity to an already complex decision making process. There are also more trivial reasons like the absence of scenarios or any kind of uncertainty forecasts. Despite these challenges, the primary advantage of stochastic optimization – its superior performance when paired with reliable probabilistic forecasts* – is undeniable. There are publications that quantify how much better stochastic optimization is compared to a deterministic approach for unit-commitment and bidding problems. However, what was somewhat unclear to me is when you actually need probabilistic forecasts and which one until I found this paper [2]. This blog post aims to concisely review the “Bidding Curve Optimization” section from the insightful paper titled “Bidding and Scheduling in Energy Markets: Which Probabilistic Forecast Do We Need?”.\n\n* It’s not that hard, check out my paper for creating time-coupled wind power scenarios [1]."
  },
  {
    "objectID": "posts/when-to-use-probabilistic-forecasts-for-bidding/when_to_use_probabilistic_forecasts_for_bidding.html#approach",
    "href": "posts/when-to-use-probabilistic-forecasts-for-bidding/when_to_use_probabilistic_forecasts_for_bidding.html#approach",
    "title": "When to use probabilistic forecasts for bidding in power markets",
    "section": "2 Approach",
    "text": "2 Approach\nThe authors dissect the question at hand by analytically examining various use cases, providing a structured approach to the problem. Before being able to do that, we need to first understand the fundamental analytical setup. Furthermore, I will streamline the approach by focusing solely on the power plant owner’s perspective, in contrast to the paper’s dual focus on both market operators and power plant owners.\nLet’s have a closer look at the mixed-integer stochastic two stage problem that is used in the paper. In the field of power market bidding, the mixed-integer stochastic two-stage problem stands as a sophisticated modeling approach, offering a nuanced method to capture the complexities of decision-making under uncertainty. This is the generic objective function, in which the parts that concern the market operator view are already removed: \\[\n\\min_{g^{\\omega}, u^{\\omega}, s^{\\omega}, V(.)} \\mathbb{E} \\left[\n\\sum_{t \\in T} \\pi^{\\omega}_{t} V_{t}(\\pi^{\\omega}_{t}) + c^{F}_{t} g^{\\omega}_{t} + c^{ST}_{t} I^{st}_{t} (u^{\\omega}_{t}, u^{\\omega}_{t-1})\n\\right]\n\\]\nThe first thing that can be noticed is that we are minimizing an expected value over all time steps in \\(T\\). The first stage decision consists of submitting a bidding curve \\(V_{t}(\\pi^{\\omega}_{t})\\) with uncertain price realizations \\(\\pi^{\\omega}_{t}\\) to the market. Next comes the fuel costs \\(c^{F}_{t}\\) which are multiplied with the plant’s power generation \\(g^{\\omega}_{t}\\). This is followed by the start-up costs \\(c^{ST}_{t}\\) as well as the function \\(I^{st}_{t}\\), which becomes 1 when the power plant starts up in \\(t\\).\nThe constraints look like this:\n\\[\\begin{aligned}\n& \\text{s.t.} \\\\\n& g^{\\omega}_{t} + V_{t}(\\pi^{\\omega}_{t}) = 0, \\\\\n& u^{\\omega}_{t} \\underline{g}_{t} \\leq g^{\\omega}_{t} \\leq u^{\\omega}_{t} \\overline{g}_{t}, \\\\\n& V_{t}(\\cdot) \\in V, \\; u^{\\omega}_{t} \\in \\{0, 1\\} \\quad \\forall t \\in T, \\quad \\text{a.e.} \\; \\omega \\in \\Omega.\n\\end{aligned}\\]\nI have omitted the storage component because I won’t look at it in more detail. The first constraint is some kind of power balance. Whatever the bidding curve is returning for a given price needs to be produced. The second constraint makes sure that the power plant is operating within its operational limits."
  },
  {
    "objectID": "posts/when-to-use-probabilistic-forecasts-for-bidding/when_to_use_probabilistic_forecasts_for_bidding.html#use-cases",
    "href": "posts/when-to-use-probabilistic-forecasts-for-bidding/when_to_use_probabilistic_forecasts_for_bidding.html#use-cases",
    "title": "When to use probabilistic forecasts for bidding in power markets",
    "section": "3 Use cases",
    "text": "3 Use cases\nAnalyzing different use cases of power plants reveals how their characteristics influence bidding strategies and the need for probabilistic forecasting. Let’s examine these scenarios, starting with the simplest case.\n\nPower plant without start-up costs\nIn this first basic case we assume that we have a power plant without start-up costs and that we simply want to bid everything we have into the market. In this particular case we can substitute our generation \\(g^{\\omega}_{t}\\) with the bidding curve \\(V_{t}(\\pi^{\\omega}_{t})\\) because we will produce whatever the market clearing is telling us. This boils down to this objective function: \\[\n\\min_{V(.)} \\sum_{t \\in T} \\mathbb{E} \\left[\n(\\pi^{\\omega}_{t} -  c^{F}_{t}) V_{t}(\\pi^{\\omega}_{t})\n\\right]\n\\] , which could be rewritten when minimizing over all scenarios \\(S\\) with their weights \\(w_s\\) and prices \\(\\pi_{t,s}\\): \\[\n\\min_{V(.)} \\sum_{s \\in S} w_s \\sum_{t \\in T}\n(\\pi_{t,s} -  c^{F}_{t}) V_{t}(\\pi_{t,s})\n\\]\nDue to the characteristics of a bidding curve \\(V(.)\\), which are that we don’t bid anything below our productions costs and all our production above, it can be concluded that we do not need a forecast for bidding in this case.\n\n\nPower plant with start-up costs\nWhen adding start-up costs our optimization problem becomes: \\[\n\\min_{V(.), \\mathbf{g}, \\mathbf{u}} \\sum_{s \\in S} w_s \\sum_{t \\in T}  \n(\\pi_{t,s} -  c^{F}_{t}) V_{t}(\\pi_{t,s}) + c^{ST} I^{ST}(u_{t,s}, u_{t-1})\n\\]\n\\[\\begin{aligned}\n& \\text{s.t.} \\\\\n& g_{ts} + V_{t}(\\pi_{t,s}) = 0, \\\\\n& u_{t,s} \\underline{g}_{t} \\leq g^{\\omega}_{t} \\leq u_{t,s} \\overline{g}_{t}, \\\\\n& V_{t}(\\cdot) \\in V, \\; u^{\\omega}_{t} \\in \\{0, 1\\} \\quad \\forall t \\in T, \\forall s \\in S.\n\\end{aligned}\\]\nIn contrast to the problem without start-up costs, we now have to deal with the integer variables \\(u_{t,s}\\) and the start-up function \\(I^{ST}\\) that links two adjacent time steps. This is also why pairwise joint predictive distributions are needed. This seems intuitive when looking at the equations at hand but I still find it interesting because basically everyone is going directly from deterministic forecasts to scenarios that were derived from the full joint distribution. However, you could also take a small step and simply estimate pairwise joint predictive functions which should be easier than for example dealing with copulas to get your scenarios right.\n\n\nPower plant with storage\nI will not get into detail here because it is rather obvious that you are going to need a full joint distribution as soon as your model has a storage. The reason is that not only adjacent time steps depend on each other but basically all. If you want to know more about this, the paper offers a detailed explanation."
  },
  {
    "objectID": "posts/when-to-use-probabilistic-forecasts-for-bidding/when_to_use_probabilistic_forecasts_for_bidding.html#bidding-with-variable-time-step-coupling",
    "href": "posts/when-to-use-probabilistic-forecasts-for-bidding/when_to_use_probabilistic_forecasts_for_bidding.html#bidding-with-variable-time-step-coupling",
    "title": "When to use probabilistic forecasts for bidding in power markets",
    "section": "4 Bidding with variable time step coupling",
    "text": "4 Bidding with variable time step coupling\nAt the end of the paper, the authors run an actual simulation in which they model a power plant with start-up costs and compare deterministic forecasts and probabilistic forecasts in combination with stochastic optimization. There are two findings that I find worth highlighting here.\n\nShare of start-up costs\nThe difference between having and not having start-up costs is \\(c^{ST} I^{ST}(u_{t,s}, u_{t-1})\\) in the objective function. Hence, as larger the share of your start-up costs of your total costs is, the more sense it makes to use scenarios. This is also one main takeaway for me. Whether stochastic optimization really yields noteworthy better results depends a lot on the input parameters, i.e. your power plant and costs.\nIn the paper, we can see diverging results when the share of start-up costs exceeds 10 %. In practical terms, this suggests that power plants with significant start-up costs might find substantial benefits in adopting stochastic optimization to manage these expenses more effectively.\nIt would have been interesting to see a similar comparison for the storage case. If the storage is rather small compared to the rest of the system, the advantage of stochastic optimization should also be smaller.\n\n\nCorrelation\nThis point is also rather obvious but worth mentioning. What the authors show in their case study is how the correlation between the prices from time step to time step is impacting the superiority of stochastic optimization. When prices are completely uncorrelated, stochastic optimization won’t help at all. The result will be as good/bad as using the mean value as a forecast. However, the higher the correlation gets, i.e. the stronger the patterns are between prices, the better the stochastic approach gets.\nThe main takeaway for me here is once again that input data matters."
  },
  {
    "objectID": "posts/when-to-use-probabilistic-forecasts-for-bidding/when_to_use_probabilistic_forecasts_for_bidding.html#summary",
    "href": "posts/when-to-use-probabilistic-forecasts-for-bidding/when_to_use_probabilistic_forecasts_for_bidding.html#summary",
    "title": "When to use probabilistic forecasts for bidding in power markets",
    "section": "5 Summary",
    "text": "5 Summary\nIf you have asked yourself when to use stochastic optimization for bidding into energy markets and what kind of probabilistic forecasts are required, you can now find the answer in this great paper [2] from Mario Beykirch, Tim Janke and Florian Steinke. They are going through different use cases and demonstrate analytically which forecast are needed. My main takeaways are:\n\nEven in the face of uncertain price outcomes, the necessity of forecasts can vary significantly, demonstrated by the case of a simple power plant without start-up costs where no forecast might be required.\nIntroducing start-up costs changes the dynamic, necessitating only pairwise joint price distributions instead of the full joint distribution, challenging the common assumption that the full joint distribution is always required.\nCorrelation matters. The higher the correlation between price hours is, the better a stochastic optimization will be.\nHave a look at the share of start-up costs or more general every term that requires to use probabilistic instead of deterministic forecasts. The lower the contribution to the objective function is (for example a low share of start-up costs), the lower the benefit of a stochastic approach will be."
  },
  {
    "objectID": "posts/when-to-use-probabilistic-forecasts-for-bidding/when_to_use_probabilistic_forecasts_for_bidding.html#bibliography",
    "href": "posts/when-to-use-probabilistic-forecasts-for-bidding/when_to_use_probabilistic_forecasts_for_bidding.html#bibliography",
    "title": "When to use probabilistic forecasts for bidding in power markets",
    "section": "6 Bibliography",
    "text": "6 Bibliography\n[1]: R. Becker, “Generation of Time-Coupled Wind Power Infeed Scenarios Using Pair-Copula Construction”, in IEEE Transactions on Sustainable Energy, vol. 9, no. 3, pp. 1298-1306, July 2018, doi: 10.1109/TSTE.2017.2782089.\n[2]: M. Beykirch, T. Janke & F. Steinke, “Bidding and Scheduling in Energy Markets: Which Probabilistic Forecast Do We Need?”, in 17th International Conference on Probabilistic Methods Applied to Power Systems (PMAPS 2022), virtual Conference, 2022, doi: https://doi.org/10.48550/arXiv.2203.13159."
  },
  {
    "objectID": "posts/how-to-read-from-entsoe-with-python/how_to_read_from_entsoe_with_python.html",
    "href": "posts/how-to-read-from-entsoe-with-python/how_to_read_from_entsoe_with_python.html",
    "title": "How to read from ENTSO-E with Python",
    "section": "",
    "text": "The ENTSO-E transparency platform serves as a comprehensive source for various types of power system data. It encompasses six primary data categories: load, generation, transmission, balancing, outages, and congestion management. Most of these categories feature different time series, such as actual or forecasted load data. In addition to the user interface on their website, the platform also offers a RESTful API for data retrieval.\nTo access data via the API, you’ll need an API key. Registration on their website is straightforward – if you haven’t already signed up, you can do so to obtain an access key."
  },
  {
    "objectID": "posts/how-to-read-from-entsoe-with-python/how_to_read_from_entsoe_with_python.html#the-entso-e-transparency-platform",
    "href": "posts/how-to-read-from-entsoe-with-python/how_to_read_from_entsoe_with_python.html#the-entso-e-transparency-platform",
    "title": "How to read from ENTSO-E with Python",
    "section": "",
    "text": "The ENTSO-E transparency platform serves as a comprehensive source for various types of power system data. It encompasses six primary data categories: load, generation, transmission, balancing, outages, and congestion management. Most of these categories feature different time series, such as actual or forecasted load data. In addition to the user interface on their website, the platform also offers a RESTful API for data retrieval.\nTo access data via the API, you’ll need an API key. Registration on their website is straightforward – if you haven’t already signed up, you can do so to obtain an access key."
  },
  {
    "objectID": "posts/how-to-read-from-entsoe-with-python/how_to_read_from_entsoe_with_python.html#entsoe-py",
    "href": "posts/how-to-read-from-entsoe-with-python/how_to_read_from_entsoe_with_python.html#entsoe-py",
    "title": "How to read from ENTSO-E with Python",
    "section": "2 entsoe-py",
    "text": "2 entsoe-py\nTo access data from the API, you could write your own code. However, be prepared for the challenging task of XML parsing. Fortunately, there is Python package called entsoe-py that I am using here to retrieve data from ENTSO-E. Before diving into that, I would like to acknowledge one of its notable contributors.\n\nContributors\nContributing to open-source projects like this one is immensely valuable to the power system modeling community and is deeply appreciated.\nOne contributor whose work has been particularly beneficial to me (among many others) is Frank Boerman. Unfortunately, I don’t know him personally, but his contributions have significantly advanced the accessibility of power system data for Python programmers and others. As of January 9, 2024, he has made 60 commits to the entsoe-py package.\nAdditionally, Frank appears to be the lead maintainer of the jao-py Python package, which facilitates access to FBMC data from JAO. He has also integrated some of this data into well-organized Grafana dashboards, which I highly recommend exploring.\nFurthermore, he runs a blog focusing on energy market-related topics, offering valuable insights and information.\n\n\nInstallation\nThe entsoe-py package is on PyPI an can be installed via pip.\npip install entsoe-py\nThe code can be found here on GitHub.\nThe latest version 0.6.2 requires pandas 1.4.0 or higher.\n\n\nPackage structure\nThe package features two primary classes: EntsoeRawClient and EntsoePandasClient. While they share the same input parameters, their key difference lies in the format of the returned data. Using EntsoeRawClient will return data in XML format, requiring users to handle their own parsing. On the other hand, EntsoePandasClient is perhaps the more convenient and widely used interface, as it integrates seamlessly with the Pandas library, simplifying data handling and analysis."
  },
  {
    "objectID": "posts/how-to-read-from-entsoe-with-python/how_to_read_from_entsoe_with_python.html#reading-data",
    "href": "posts/how-to-read-from-entsoe-with-python/how_to_read_from_entsoe_with_python.html#reading-data",
    "title": "How to read from ENTSO-E with Python",
    "section": "3 Reading data",
    "text": "3 Reading data\n\nInitializing the Pandas client\nTo begin, let’s read data from the ENTSO-E transparency platform. In the example below, I’ll demonstrate using the dotenv library to import my ENTSO-E API key from environment variables. This approach is particularly useful for keeping sensitive information secure. If you’re running your code in a local environment and prefer simplicity, you can directly insert your API key into the code. However, be cautious with this method, especially if sharing your code, as it exposes your key to potential security risks.\n\nimport os\n\nimport pandas as pd\nfrom dotenv import load_dotenv\nfrom entsoe import EntsoePandasClient\n\nload_dotenv()\nENTSOE_API_KEY = os.environ[\"ENTSOE_API_KEY\"]\nclient = EntsoePandasClient(api_key=ENTSOE_API_KEY)\n\nTo retrieve specific time series data from the ENTSO-E platform, it’s crucial to call the corresponding function in the entsoe-py package. Each time series data type, like load forecasts or day-ahead prices, has a dedicated function – for instance, query_load_forecast() for load forecasts and query_day_ahead_prices() for day-ahead prices. You can find a comprehensive list of these functions on the entsoe-py’s GitHub page.\nIn the following example, I demonstrate how to retrieve the day-ahead price data for Germany spanning three and a half years. It’s important to note that querying data over extended periods, such as more than one year, can be time-consuming due to the necessary pagination. Be prepared for this process to take several seconds.\n\nprices = client.query_day_ahead_prices(\n    country_code=\"DE_LU\",\n    start=pd.Timestamp(\"2020-01-01\", tz=\"CET\"),\n    end=pd.Timestamp(\"2023-06-01\", tz=\"CET\"),\n)\n\nWhat we get back is a pandas Series.\n\ntype(prices)\n\npandas.core.series.Series\n\n\nThe index is a proper pandas DatetimeIndex with the correct time zone, which is quite useful when merging with other time series.\n\ntype(prices.index)\n\npandas.core.indexes.datetimes.DatetimeIndex\n\n\nLet’s have a sanity check and plot these prices.\n\nprices.plot()\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "posts/how-to-read-from-entsoe-with-python/how_to_read_from_entsoe_with_python.html#summary",
    "href": "posts/how-to-read-from-entsoe-with-python/how_to_read_from_entsoe_with_python.html#summary",
    "title": "How to read from ENTSO-E with Python",
    "section": "4 Summary",
    "text": "4 Summary\nThe ENTSO-E transparency platform stands out as a crucial source for open power system data, catering to a wide range of needs within the power system modeling community. The entsoe-py Python package significantly simplifies the process of accessing this data through the ENTSO-E API. Credit goes to contributors like Frank Boerman, whose efforts have led to the development of user-friendly interfaces such as EntsoePandasClient. This particular interface enables the seamless integration of power system data directly into Pandas Series or DataFrames, enhancing the ease and efficiency of data analysis and handling"
  },
  {
    "objectID": "posts/how-to-read-from-entsoe-with-python/how_to_read_from_entsoe_with_python.html#links",
    "href": "posts/how-to-read-from-entsoe-with-python/how_to_read_from_entsoe_with_python.html#links",
    "title": "How to read from ENTSO-E with Python",
    "section": "5 Links",
    "text": "5 Links\nCheck out the user guide for the Transparency Platform RESTful API if you want to dig deeper.\nHere’s the link to the entsoe-py’s GitHub page again, for quick access"
  },
  {
    "objectID": "posts/why-polars-is-great/why_polars_is_great.html",
    "href": "posts/why-polars-is-great/why_polars_is_great.html",
    "title": "Why Polars is great",
    "section": "",
    "text": "According to the official Polars website, Polars is\n\n[…] a lightning fast DataFrame library/in-memory query engine. Its embarrassingly parallel execution, cache efficient algorithms and expressive API makes it perfect for efficient data wrangling, data pipelines, snappy APIs and so much more.\n\nThis sounds a lot like pandas, doesn’t it? According to the pandas website, pandas is:\n\n[…] a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\n\nAlright then, both are clearly intended for similar things, but Polars has this emphasize on speed and this is really the first noteworthy difference. Polars is implemented in Rust, which, in contrast to Python, does not have a Global Interpreter Lock (GIL). That means that you can use your entire machine for your data wrangling with Polars. Let’s have a closer look into the differences of these two tools."
  },
  {
    "objectID": "posts/why-polars-is-great/why_polars_is_great.html#what-is-polars",
    "href": "posts/why-polars-is-great/why_polars_is_great.html#what-is-polars",
    "title": "Why Polars is great",
    "section": "",
    "text": "According to the official Polars website, Polars is\n\n[…] a lightning fast DataFrame library/in-memory query engine. Its embarrassingly parallel execution, cache efficient algorithms and expressive API makes it perfect for efficient data wrangling, data pipelines, snappy APIs and so much more.\n\nThis sounds a lot like pandas, doesn’t it? According to the pandas website, pandas is:\n\n[…] a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\n\nAlright then, both are clearly intended for similar things, but Polars has this emphasize on speed and this is really the first noteworthy difference. Polars is implemented in Rust, which, in contrast to Python, does not have a Global Interpreter Lock (GIL). That means that you can use your entire machine for your data wrangling with Polars. Let’s have a closer look into the differences of these two tools."
  },
  {
    "objectID": "posts/why-polars-is-great/why_polars_is_great.html#the-main-differences-between-polars-and-pandas",
    "href": "posts/why-polars-is-great/why_polars_is_great.html#the-main-differences-between-polars-and-pandas",
    "title": "Why Polars is great",
    "section": "2 The main differences between Polars and pandas",
    "text": "2 The main differences between Polars and pandas\n\nThere is no index/multi-index in Polars\nYes, you read this correctly. How can that be true? Could you have wasted all the hours you spent learning how to use reset_index(), set_index(), and the difference between .loc[] and .iloc[]? Maybe. Honestly, if you really think about it, it doesn’t make too much sense. Just think about databases and SQL for a moment. SQL tables have no such index either and SQL is the forefront of data engineering. On the other hand, once you have mastered pandas’ index, functions like resample() can make your life much easier. But once again, at the end, it doesn’t matter really whether it is an index or simply a column. Polars makes a good point with this:\n\nPolars aims to have predictable results and readable queries, as such we think an index does not help us reach that objective.\n\n\n\nParallel operations\nAs already mentioned, you can parallelize with Polars because it is written in Rust. This all happens under hood. That means you don’t need to care about it or install anything else.\n\n\nThe lazy API\nAs of today, pandas has only an eager API. This means that when you run whatever command with pandas, let’s say join(), pandas will execute this directly. Polars lets you collect commands into a query, which are then executed when you call the collect() function. By knowing all the steps you want to execute, Polars can make use of a query optimization to speed up your code."
  },
  {
    "objectID": "posts/why-polars-is-great/why_polars_is_great.html#what-they-have-in-common",
    "href": "posts/why-polars-is-great/why_polars_is_great.html#what-they-have-in-common",
    "title": "Why Polars is great",
    "section": "3 What they have in common",
    "text": "3 What they have in common\nA lot actually. That is because pandas is an awesome library and they have thought of great things that do not need to be reinvented. If you are coming from pandas, you will need some time to adjust yourself to the somewhat different syntax. Other than that, you will quickly find your way around things in Polars.\nIn this section of the Polars’ user guide, you can find a list of the key syntax differences."
  },
  {
    "objectID": "posts/why-polars-is-great/why_polars_is_great.html#more-amazing-features",
    "href": "posts/why-polars-is-great/why_polars_is_great.html#more-amazing-features",
    "title": "Why Polars is great",
    "section": "4 More amazing features",
    "text": "4 More amazing features\nThis is a list of features I find personally very interesting and that are difficult to find elsewhere.\n\nScanning files\nMost input functions that start with read_ have also a scan_ equivalent, e.g. read_parquet() and scan_parquet(). What it does is that it allows you to first scan the file instead of reading the whole file into memory. This gives you to the possibility to query data in a file with less RAM and CPU usage.\n\n\nSQL\nIf you are familiar with SQL and want to use it even in the Python world, you can do this with Polars. Polars offers an SQL API that lets you write SQL queries. This even works across different files with different file formats. If you combine it with scanning files, you can for example merge parts of a csv file with a couple of columns in a parquet file on a cloud storage without consuming too much memory or CPU.\nThis is the link to the SQL section of the Polars user guide.\n\n\nStreaming\nEven though the streaming feature is still under development, you can already use it for a few file formats and functions. Streaming is running your query in batches, which enables you to deal with datasets that are larger than your memory.\nYou can read more about streaming in the official user guide."
  },
  {
    "objectID": "posts/why-polars-is-great/why_polars_is_great.html#should-i-switch-completely-to-polars",
    "href": "posts/why-polars-is-great/why_polars_is_great.html#should-i-switch-completely-to-polars",
    "title": "Why Polars is great",
    "section": "5 Should I switch completely to Polars?",
    "text": "5 Should I switch completely to Polars?\nNo, don’t! Especially not if you have already thousands of lines of pandas code. Instead, use Polars there you really need it, namely where your bottlenecks are. So start there where you really need a speed-up or where reading from many different data sources has become a burden with pandas. In summary, use pandas and Polars at what they are good at, instead of going for one of them only.\nIn your code, you can simply convert between these two DataFrame types, even though you shouldn’t do it too often with big DataFrames. Here is an example:\n\nimport pandas as pd\nimport polars as pl\n\npandas_df = pd.DataFrame({\"col1\": [0, 1], \"col2\": [\"a\", \"b\"]})\ntype(pandas_df)\n\npandas.core.frame.DataFrame\n\n\n\npolars_df = pl.from_pandas(pandas_df)\ntype(polars_df)\n\npolars.dataframe.frame.DataFrame\n\n\n\ntype(polars_df.to_pandas())\n\npandas.core.frame.DataFrame"
  },
  {
    "objectID": "posts/why-polars-is-great/why_polars_is_great.html#summary",
    "href": "posts/why-polars-is-great/why_polars_is_great.html#summary",
    "title": "Why Polars is great",
    "section": "6 Summary",
    "text": "6 Summary\nPolars is , similar to pandas, a powerful Python libary for data wrangling and more. Given that it is written in Rust, it is much faster and more memory-efficient than pandas. If you need that extra bit of performance, you should definetly check out Polars.\nHowever, don’t be silly and rewrite all your pandas code in Polars. Polars and pandas get along with each other and are probably most powerful when used together."
  },
  {
    "objectID": "posts/why-polars-is-great/why_polars_is_great.html#links",
    "href": "posts/why-polars-is-great/why_polars_is_great.html#links",
    "title": "Why Polars is great",
    "section": "7 Links",
    "text": "7 Links\nCheck out the official Polars website. It contains a nice user guide and the API docs. If you are coming from pandas, read Coming from Pandas first before you start writing code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "spotopt-blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nWhen to use probabilistic forecasts for bidding in power markets\n\n\n\n\n\nUnraveling the complexities of using probabilistic forecasts in power market bidding, this post reviews a pivotal study by Mario Beykirch, Tim Janke and Florian Steinke, highlighting how start-up costs, price correlations, and power plant characteristics influence which type of forecast is optimal.\n\n\n\n\n\n\nJan 25, 2024\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nHow to read from ENTSO-E with Python\n\n\n\n\n\nThe ENTSO-E transparency platform is an invaluable resource for accessing diverse power system data through a user interface and a RESTful API. This blog post introduces the entsoe-py Python package, a tool designed to streamline data retrieval from the ENTSO-E API.\n\n\n\n\n\n\nJan 18, 2024\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nWhy Polars is great\n\n\n\n\n\nDiscover the capabilities of Polars and explore its key differences from pandas in data manipulation and analysis. Learn how to choose between these powerful tools based on your data handling needs, ensuring efficient and effective data analysis.\n\n\n\n\n\n\nDec 1, 2023\n\n\n5 min\n\n\n\n\n\n\nNo matching items"
  }
]